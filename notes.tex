%! Author = Kevin Lin
%! Date = 9/30/2025

% Preamble
\documentclass[11pt,a4paper,margin=1in]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}

\title{Notes}
\author{Kevin Lin}
\date{9/30/2025}

% Document
\begin{document}
\maketitle
\section{}
\begin{gather*}
\forall A \in \mathbb{R}\\
\frac{\frac{1}{8}}{\frac{1}{2} + \frac{1}{8}}\\
\dfrac{1}{2}\\
\Omega\\
\omega\\
\end{gather*}
\begin{enumerate}[A.]
    \item A
    \item B
\end{enumerate}

\section{Extended Monty Hall Problem}
\begin{flushleft}
    Suppose you have $n$ doors, where behind 1 is the car and behind $n - 1$ are
    goats. Monty Hall will open $k ([0, n - 2])$ doors as he has to leave one door
    unopened for you to switch to, and the original door you picked. The chance
    that you picked the car originally is $\frac{1}{n}$, hence the chance you didn't
    pick the car is $\frac{n - 1}{n}$. When Monty Hall opens $k$ doors, the probability
    that you should switch to win is now determined by the probability you didn't 
    pick the correct door the first times multiplied by the new probability that
    you pick the correct door when you switch, given by:
\end{flushleft}
\begin{gather*}
    \frac{n - 1}{n} \cdot \frac{1}{n - k - 1} = \frac{1}{n} \cdot \frac{n - 1}{n - k - 1}
\end{gather*}
\begin{flushleft}
    Suppose Monty Hall opens $n - 2$ doors, then when you switch the probablity
    of winning becomes apparent:
\end{flushleft}
\begin{gather*}
    \frac{n - 1}{n} \cdot \frac{1}{n - (n - 2) - 1} = \frac{n - 1}{n}
\end{gather*}
\begin{flushleft}
    In the standard Monty Hall problem where $n = 3$ and $k = 1$, the probability 
    of winning when you switch is:
\end{flushleft}
\begin{gather*}
    \frac{3 - 1}{3} \cdot \frac{1}{3 - 1 - 1} = \frac{2}{3}
\end{gather*}

\section{Bayes}
\begin{flushleft}
    In a channel, you can send either a 0 or 1. The probability of sending a 0 is 
    $\frac{2}{3}$ and the probability of sending a 1 is $\frac{1}{3}$. The probability
    that a number is received correctly as is $\frac{3}{4}$ and incorrectly is
    is $\frac{1}{4}$. What is P(1 sent | 1 received)?
\end{flushleft}
\begin{gather*}
    P(1 \text{ sent} | 1 \text{ received}) = \frac{P(1 \text{ received} | 1 \text{ sent}) \cdot P(1 \text{ sent})}{P(1 \text{ received})}\\
    = \frac{\frac{3}{4} \cdot \frac{1}{3}}{\frac{2}{3} \cdot \frac{1}{4} + \frac{1}{3} \cdot \frac{3}{4}}
    = \frac{\frac{1}{4}}{\frac{1}{6} + \frac{1}{4}}
    = \frac{3}{5}
\end{gather*}

\section{(Cumulative) Distribution Function}
\begin{flushleft}
    The probability that $X(\omega)$ resides in the interval $(-\infty, x]$ depends 
    on only the right endpoint $x$, hence it is convenient to:
\end{flushleft}
\begin{gather*}
    F : x \mapsto P(X \leq x) \\
    F(x) = P(X \leq x) = P({\omega : X(\omega) \leq x}) = P(X^{-1}(-\infty, x])\\
\end{gather*}
\begin{flushleft}
    Properties of $F$:
\end{flushleft}
\begin{enumerate}[1.]
    \item Right continuous
    \item Increase monotonically
    \item Has limits $F(-\infty) = 0$ and $F(\infty) = 1$
    \item 
        $P(a < X \leq b) = F(b) - F(a)$ \\
        $P((a, b]) = P((-\infty, b]) - P((-\infty, a])$ \\
\end{enumerate}
\begin{itemize}
    \item Determines probabilities of all types of intervals
    \item Once the CDF is known, the original probability space is in the background
    \item From this point, we can just deal with $F$, as if it were a real-line
    sample space governed by the distribution $F$
\end{itemize}
\begin{flushleft}
    When the number of times $F(X-) \neq F(x)$ is countable, we say $X$ is a 
    discrete random variable. Note that intervals $F(x) - F(x-)$ are all disjoint
    at each point of a jump, therefore must contain a rational number of events
    within.
\end{flushleft}
\subsection{Discrete Distribution}
\begin{flushleft}
    If we can enumerate the point of jumps for any distribution function, we can
    define the probability of each jump point and hence achieve a discrete distribution.\\

    Let $u(x)$ be a unit step function, and $u(x) = 1$ if $x \geq 0$, else 0.\\
    $F$ is a \textit{discrete distribution} if it can be represented as: 
\end{flushleft}
\begin{gather*}
    F(x) = \sum_{j} p_j u(x - x_j) \\
\end{gather*}
\begin{flushleft}
    where ${x_j}$ is a countable set of real numbers, $p_j > 0$ are such that 
    $\sum_j p_j = 1$.
\end{flushleft}
\subsection{Heaviside Distribution}
\begin{flushleft}
    $F(x) = u(x - x_0)$ is a discrete distribution with a fixed value $x_0$ with
    probability 1. Hence $P(X \geq x_0) = 1$ and $P(X < x_0) = 0$. Standard value
    for $x_0$ is 0. Can use indicator function to define such.
\end{flushleft}

\section{Moments}
Let $X$ be a random variable given the following distribution:
\begin{gather*}
    X = \begin{cases} 0, \quad 1 - p \\ 1, \quad p \end{cases} \\
    E[X^2] = 0^2 \cdot (1 - p) + 1^2 \cdot p = p \\
    E[X] = p \\
    Var(X) = E[X^2] - (E[X])^2 = p - p^2 = p(1 - p) \\
\end{gather*}
Now let $X$ be a random variable following a uniform distribution:
\begin{gather*}
    f_X(x) = \begin{cases} \frac{1}{b - a}, \quad a \leq x \leq b \\ 0, \quad else \end{cases} \\
    E[X] = \int_{a}^{b} x f_X(x) dx = \int_{a}^{b} x \cdot \frac{1}{b - a} dx = \frac{1}{b - a} \cdot \frac{x^2}{2} \bigg|_{a}^{b} \\
    = \frac{1}{b - a} \cdot \left( \frac{b^2}{2} - \frac{a^2}{2} \right) = \frac{b^2 - a^2}{2(b - a)} = \frac{b + a}{2} \\
    E[X^2] = \int_{a}^{b} x^2 f_X(x) dx = \int_{a}^{b} x^2 \cdot \frac{1}{b - a} dx = \frac{1}{b - a} \cdot \frac{x^3}{3} \bigg|_{a}^{b} \\
    = \frac{1}{b - a} \cdot \left( \frac{b^3}{3} - \frac{a^3}{3} \right) = \frac{b^3 - a^3}{3(b - a)} = \frac{(b - a)(b^2 + ab + a^2)}{3(b - a)} = \frac{b^2 + ab + a^2}{3} \\
    Var(X) = E[X^2] - (E[X])^2 = \frac{b^2 + ab + a^2}{3} - \left( \frac{b + a}{2} \right)^2 \\
    = \frac{b^2 + ab + a^2}{3} - \frac{b^2 + 2ab + a^2}{4} = \frac{4(b^2 + ab + a^2) - 3(b^2 + 2ab + a^2)}{12} \\
    = \frac{b^2 - 2ab + a^2}{12} = \frac{(b - a)^2}{12}
\end{gather*}
\subsection{Moment Generating Functions}
\begin{gather*}
    \psi_X(t) = E[e^{tX}] = \int e^{tx} f_X(x) dx\\
    \psi_X(t) = E[1 + tX + \frac{t^2 X^2}{2!} + \frac{t^3 X^3}{3!} + ...] \\
    = 1 + tE[X] + \frac{t^2 E[X^2]}{2!} + \frac{t^3 E[X^3]}{3!} + ... \\
    = \sum_{n=0}^{\infty} \frac{t^n E[X^n]}{n!}
    \psi_X(0) = 1 \\
    \psi_X'(t) = \sum_{n=1}^{\infty} \frac{t^{n-1} E[X^n]}{(n-1)!} \\
    \psi_X'(0) = E[X] \\
    \psi_X''(t) = \sum_{n=2}^{\infty} \frac{t^{n-2} E[X^n]}{(n-2)!} \\
    \psi_X''(0) = E[X^2] \therefore \\
    \psi_X^{(k)}(0) = E[X^k]
\end{gather*}

\end{document}